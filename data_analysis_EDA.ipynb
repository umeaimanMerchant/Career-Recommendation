{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aad809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5807dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data set\n",
    "df = pd.read_csv(\"data/concatenated_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b6b853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>country_code</th>\n",
       "      <th>current_company:company_id</th>\n",
       "      <th>current_company:name</th>\n",
       "      <th>position</th>\n",
       "      <th>followers</th>\n",
       "      <th>about</th>\n",
       "      <th>...</th>\n",
       "      <th>education</th>\n",
       "      <th>avatar</th>\n",
       "      <th>languages</th>\n",
       "      <th>certifications</th>\n",
       "      <th>recommendations</th>\n",
       "      <th>recommendations_count</th>\n",
       "      <th>volunteer_experience</th>\n",
       "      <th>courses</th>\n",
       "      <th>connections</th>\n",
       "      <th>job_role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>sheharyar-chaudhary-325b3465</td>\n",
       "      <td>Sheharyar Chaudhary</td>\n",
       "      <td>Dallas, Texas, United States</td>\n",
       "      <td>US</td>\n",
       "      <td>avantlink</td>\n",
       "      <td>AvantLink</td>\n",
       "      <td>Data Engineer || Google Certified Data Analyst...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>With over 7 years of extensive hands-on experi...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"degree\":\"Bachelor of Science (BS)\",\"end_yea...</td>\n",
       "      <td>https://static.licdn.com/aero-v1/sc/h/1c5u578i...</td>\n",
       "      <td>[{\"subtitle\":\"Full professional proficiency\",\"...</td>\n",
       "      <td>[{\"meta\":\"Issued Jun 2023 See credential\",\"sub...</td>\n",
       "      <td>Umer Siddiqui “I had the privilege of working ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>byby-louise-a91456243</td>\n",
       "      <td>Byby Louise</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "      <td>US</td>\n",
       "      <td>fiscalnote</td>\n",
       "      <td>FiscalNote</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"degree\":\"Master's degree\",\"end_year\":\"2014\"...</td>\n",
       "      <td>https://media.licdn.com/dms/image/C4E03AQEYsL5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>larry-richardson-jr</td>\n",
       "      <td>Larry Richardson Jr.</td>\n",
       "      <td>Frederick, Maryland, United States</td>\n",
       "      <td>US</td>\n",
       "      <td>ncma</td>\n",
       "      <td>National Contract Management Association (NCMA)</td>\n",
       "      <td>Infrastructure as Code | Data Analysis | Data ...</td>\n",
       "      <td>243.0</td>\n",
       "      <td>I caught fire with cloud computing. About eigh...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"degree\":\"Data Analytics/Engineering\",\"meta\"...</td>\n",
       "      <td>https://static.licdn.com/aero-v1/sc/h/1c5u578i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"meta\":\"Issued Sep 2023\",\"subtitle\":\"Associa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-13</td>\n",
       "      <td>thomas-roussos</td>\n",
       "      <td>Thomas Roussos</td>\n",
       "      <td>Massapequa</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Student at State University of New York Colleg...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Hello! My name is Thomas Roussos and I am a st...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"end_year\":\"2018\",\"start_year\":\"2014\",\"title...</td>\n",
       "      <td>https://media.licdn.com/dms/image/C5603AQGviES...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-12-13</td>\n",
       "      <td>oceanox</td>\n",
       "      <td>XIN OU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>transsnetfinancial</td>\n",
       "      <td>Transsnet Financial</td>\n",
       "      <td>Risk Control Data Analyst at Transsnet Financi...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>I am an MBA in Business Analytics student who ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"degree\":\"Master of Business Administration ...</td>\n",
       "      <td>https://media.licdn.com/dms/image/C4D03AQEvZ-L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp                            id                  name  \\\n",
       "0  2023-11-06  sheharyar-chaudhary-325b3465   Sheharyar Chaudhary   \n",
       "1  2023-11-06         byby-louise-a91456243           Byby Louise   \n",
       "2  2023-11-06           larry-richardson-jr  Larry Richardson Jr.   \n",
       "3  2023-12-13                thomas-roussos        Thomas Roussos   \n",
       "4  2023-12-13                       oceanox                XIN OU   \n",
       "\n",
       "                                 city country_code current_company:company_id  \\\n",
       "0        Dallas, Texas, United States           US                  avantlink   \n",
       "1   New York, New York, United States           US                 fiscalnote   \n",
       "2  Frederick, Maryland, United States           US                       ncma   \n",
       "3                          Massapequa           US                        NaN   \n",
       "4                                 NaN           US         transsnetfinancial   \n",
       "\n",
       "                              current_company:name  \\\n",
       "0                                        AvantLink   \n",
       "1                                       FiscalNote   \n",
       "2  National Contract Management Association (NCMA)   \n",
       "3                                              NaN   \n",
       "4                              Transsnet Financial   \n",
       "\n",
       "                                            position  followers  \\\n",
       "0  Data Engineer || Google Certified Data Analyst...     2000.0   \n",
       "1                                       Data Analyst      112.0   \n",
       "2  Infrastructure as Code | Data Analysis | Data ...      243.0   \n",
       "3  Student at State University of New York Colleg...       15.0   \n",
       "4  Risk Control Data Analyst at Transsnet Financi...       31.0   \n",
       "\n",
       "                                               about  ...  \\\n",
       "0  With over 7 years of extensive hands-on experi...  ...   \n",
       "1                                                NaN  ...   \n",
       "2  I caught fire with cloud computing. About eigh...  ...   \n",
       "3  Hello! My name is Thomas Roussos and I am a st...  ...   \n",
       "4  I am an MBA in Business Analytics student who ...  ...   \n",
       "\n",
       "                                           education  \\\n",
       "0  [{\"degree\":\"Bachelor of Science (BS)\",\"end_yea...   \n",
       "1  [{\"degree\":\"Master's degree\",\"end_year\":\"2014\"...   \n",
       "2  [{\"degree\":\"Data Analytics/Engineering\",\"meta\"...   \n",
       "3  [{\"end_year\":\"2018\",\"start_year\":\"2014\",\"title...   \n",
       "4  [{\"degree\":\"Master of Business Administration ...   \n",
       "\n",
       "                                              avatar  \\\n",
       "0  https://static.licdn.com/aero-v1/sc/h/1c5u578i...   \n",
       "1  https://media.licdn.com/dms/image/C4E03AQEYsL5...   \n",
       "2  https://static.licdn.com/aero-v1/sc/h/1c5u578i...   \n",
       "3  https://media.licdn.com/dms/image/C5603AQGviES...   \n",
       "4  https://media.licdn.com/dms/image/C4D03AQEvZ-L...   \n",
       "\n",
       "                                           languages  \\\n",
       "0  [{\"subtitle\":\"Full professional proficiency\",\"...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                      certifications  \\\n",
       "0  [{\"meta\":\"Issued Jun 2023 See credential\",\"sub...   \n",
       "1                                                NaN   \n",
       "2  [{\"meta\":\"Issued Sep 2023\",\"subtitle\":\"Associa...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                     recommendations recommendations_count  \\\n",
       "0  Umer Siddiqui “I had the privilege of working ...                   9.0   \n",
       "1                                                NaN                   NaN   \n",
       "2                                                NaN                   NaN   \n",
       "3                                                NaN                   NaN   \n",
       "4                                                NaN                   NaN   \n",
       "\n",
       "  volunteer_experience courses connections       job_role  \n",
       "0                  NaN     NaN         NaN  Data Analysis  \n",
       "1                  NaN     NaN         NaN  Data Analysis  \n",
       "2                  NaN     NaN         NaN  Data Analysis  \n",
       "3                  NaN     NaN        15.0  Data Analysis  \n",
       "4                  NaN     NaN        31.0  Data Analysis  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09611ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data Analysis             1000\n",
       "Manager                   1000\n",
       "Project Manager           1000\n",
       "Research & Development    1000\n",
       "Software Engineer         1000\n",
       "UI/UX designer             649\n",
       "Name: job_role, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['job_role'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e65789ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'id', 'name', 'city', 'country_code',\n",
       "       'current_company:company_id', 'current_company:name', 'position',\n",
       "       'followers', 'about', 'posts', 'groups', 'current_company',\n",
       "       'experience', 'url', 'people_also_viewed', 'educations_details',\n",
       "       'education', 'avatar', 'languages', 'certifications', 'recommendations',\n",
       "       'recommendations_count', 'volunteer_experience', 'courses',\n",
       "       'connections', 'job_role'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c92be9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5649 entries, 0 to 5648\n",
      "Data columns (total 27 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   timestamp                   5649 non-null   object \n",
      " 1   id                          5649 non-null   object \n",
      " 2   name                        5649 non-null   object \n",
      " 3   city                        5530 non-null   object \n",
      " 4   country_code                5649 non-null   object \n",
      " 5   current_company:company_id  3911 non-null   object \n",
      " 6   current_company:name        4776 non-null   object \n",
      " 7   position                    5649 non-null   object \n",
      " 8   followers                   3806 non-null   float64\n",
      " 9   about                       3514 non-null   object \n",
      " 10  posts                       709 non-null    object \n",
      " 11  groups                      142 non-null    object \n",
      " 12  current_company             5649 non-null   object \n",
      " 13  experience                  5429 non-null   object \n",
      " 14  url                         5649 non-null   object \n",
      " 15  people_also_viewed          4980 non-null   object \n",
      " 16  educations_details          4844 non-null   object \n",
      " 17  education                   5067 non-null   object \n",
      " 18  avatar                      5350 non-null   object \n",
      " 19  languages                   1120 non-null   object \n",
      " 20  certifications              1301 non-null   object \n",
      " 21  recommendations             803 non-null    object \n",
      " 22  recommendations_count       803 non-null    float64\n",
      " 23  volunteer_experience        749 non-null    object \n",
      " 24  courses                     241 non-null    object \n",
      " 25  connections                 860 non-null    float64\n",
      " 26  job_role                    5649 non-null   object \n",
      "dtypes: float64(3), object(24)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9fa803",
   "metadata": {},
   "source": [
    "### Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e05dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['courses','about', 'experience','certifications','job_role']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1142de0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>courses</th>\n",
       "      <th>about</th>\n",
       "      <th>experience</th>\n",
       "      <th>certifications</th>\n",
       "      <th>job_role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>With over 7 years of extensive hands-on experi...</td>\n",
       "      <td>[{\"description\":\"As a Data Engineer at AvantLi...</td>\n",
       "      <td>[{\"meta\":\"Issued Jun 2023 See credential\",\"sub...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"description\":\"Use data mining software and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I caught fire with cloud computing. About eigh...</td>\n",
       "      <td>[{\"duration\":\"Apr 2023 - Present 8 months\",\"du...</td>\n",
       "      <td>[{\"meta\":\"Issued Sep 2023\",\"subtitle\":\"Associa...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hello! My name is Thomas Roussos and I am a st...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I am an MBA in Business Analytics student who ...</td>\n",
       "      <td>[{\"description\":\"Shenzhen, Guangdong, China - ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  courses                                              about  \\\n",
       "0     NaN  With over 7 years of extensive hands-on experi...   \n",
       "1     NaN                                                NaN   \n",
       "2     NaN  I caught fire with cloud computing. About eigh...   \n",
       "3     NaN  Hello! My name is Thomas Roussos and I am a st...   \n",
       "4     NaN  I am an MBA in Business Analytics student who ...   \n",
       "\n",
       "                                          experience  \\\n",
       "0  [{\"description\":\"As a Data Engineer at AvantLi...   \n",
       "1  [{\"description\":\"Use data mining software and ...   \n",
       "2  [{\"duration\":\"Apr 2023 - Present 8 months\",\"du...   \n",
       "3                                                NaN   \n",
       "4  [{\"description\":\"Shenzhen, Guangdong, China - ...   \n",
       "\n",
       "                                      certifications       job_role  \n",
       "0  [{\"meta\":\"Issued Jun 2023 See credential\",\"sub...  Data Analysis  \n",
       "1                                                NaN  Data Analysis  \n",
       "2  [{\"meta\":\"Issued Sep 2023\",\"subtitle\":\"Associa...  Data Analysis  \n",
       "3                                                NaN  Data Analysis  \n",
       "4                                                NaN  Data Analysis  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f29695d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "courses           5408\n",
       "about             2135\n",
       "experience         220\n",
       "certifications    4348\n",
       "job_role             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1460b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 'Skillset' column\n",
    "df1['Skillset'] = df1['courses'].fillna('')+ ' '+ df1['about'].fillna('') + ' ' + df1['experience'].fillna('') + ' ' + df1['certifications'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d02d6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5649, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop all the records with skillset as null\n",
    "df1.dropna(subset = [\"Skillset\"], inplace =True)\n",
    "df1 = df1[['Skillset', 'job_role']]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf1692bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIMAN\\AppData\\Local\\Temp\\ipykernel_18476\\2695410711.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df1[\"Skillset\"] = df1[\"Skillset\"].str.replace(r'[^a-zA-Z0-9\\s]','')\n"
     ]
    }
   ],
   "source": [
    "# remove special Character\n",
    "df1[\"Skillset\"] = df1[\"Skillset\"].str.replace(r'[^a-zA-Z0-9\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c4357ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skillset</th>\n",
       "      <th>job_role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With over 7 years of extensive handson experi...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>descriptionUse data mining software and arti...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I caught fire with cloud computing About eigh...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello My name is Thomas Roussos and I am a st...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am an MBA in Business Analytics student who...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Skillset       job_role\n",
       "0   With over 7 years of extensive handson experi...  Data Analysis\n",
       "1    descriptionUse data mining software and arti...  Data Analysis\n",
       "2   I caught fire with cloud computing About eigh...  Data Analysis\n",
       "3   Hello My name is Thomas Roussos and I am a st...  Data Analysis\n",
       "4   I am an MBA in Business Analytics student who...  Data Analysis"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2957cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write it to csv file\n",
    "df1.to_csv(\"data/cleaned_data.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd04d4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skillset</th>\n",
       "      <th>job_role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With over 7 years of extensive handson experi...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>descriptionUse data mining software and arti...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I caught fire with cloud computing About eigh...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello My name is Thomas Roussos and I am a st...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am an MBA in Business Analytics student who...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Skillset       job_role\n",
       "0   With over 7 years of extensive handson experi...  Data Analysis\n",
       "1    descriptionUse data mining software and arti...  Data Analysis\n",
       "2   I caught fire with cloud computing About eigh...  Data Analysis\n",
       "3   Hello My name is Thomas Roussos and I am a st...  Data Analysis\n",
       "4   I am an MBA in Business Analytics student who...  Data Analysis"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/cleaned_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ea9430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_skills(text):\n",
    "    doc = nlp(text)\n",
    "    # Extract unique lemmatized tokens as skills\n",
    "    #skills = list(set(token.lemma_ for token in doc if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\"))\n",
    "    ner_skills = [ent.text for ent in doc.ents if ent.label_ == \"SKILL\"]\n",
    "    print(ner_skills)\n",
    "    return ner_skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfbf9a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    With over 7 years of extensive hands-on experi...\n",
       "1     [{\"description\":\"Use data mining software and...\n",
       "2    I caught fire with cloud computing. About eigh...\n",
       "3    Hello! My name is Thomas Roussos and I am a st...\n",
       "4    I am an MBA in Business Analytics student who ...\n",
       "5     [{\"company\":\"House of Providence\",\"company_id...\n",
       "Name: skillset, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.loc[0:5,\"skillset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "764e4132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"I have experience in Python, Java, and machine learning.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract skills using NER\n",
    "skills = [ent.text for ent in doc.ents if ent.label_ == \"SKILL\"]\n",
    "\n",
    "# Print the extracted skills\n",
    "print(\"Extracted Skills:\", skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28bfa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the extract_skills function to each row in the 'Skillset' column\n",
    "df['skillset'] = df['skillset'].apply(extract_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "87a6e0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['running', 'runs', 'ran', 'coding', ',', 'code', ',', 'i']\n",
      "Lemmatized: ['running', 'run', 'ran', 'coding', ',', 'code', ',', 'i']\n",
      "Stemmed: ['run', 'run', 'ran', 'code', ',', 'code', ',', 'i']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Lemmatization aims to reduce a word to its base or root form, known as the lemma. The lemma represents the canonical, dictionary form of a word.\n",
    "It involves considering the context of the word and converting it to its base form, which often corresponds to a valid word.\"\"\"\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"running runs ran coding, code, i \"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Stemming-- performes better\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Lemmatized:\", lemmatized_words)\n",
    "print(\"Stemmed:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ae7d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens (without stop words): ['experience', 'Python', ',', 'Java', ',', 'machine', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "# don't want to seperate 1 skillds like machine learning into 2\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"I have experience in Python, Java, and machine learning.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Print the filtered tokens\n",
    "print(\"Filtered Tokens (without stop words):\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c113853",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34b5f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy\n",
    "from spacy.pipeline.textcat import Config, single_label_cnn_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0747496b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the 'Skillset' column to string\n",
    "df['Skillset'] = df['Skillset'].astype(\"str\")\n",
    "df['job_role'] = df['job_role'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6cdf477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5649 entries, 0 to 5648\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   Skillset  5649 non-null   object  \n",
      " 1   job_role  5649 non-null   category\n",
      "dtypes: category(1), object(1)\n",
      "memory usage: 50.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb691a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if token.text not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['Skillset'] = df['Skillset'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d186af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the 'Skillset' column\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['Skillset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "488e9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a01e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF-IDF matrix to a file\n",
    "joblib.dump(X, 'tfidf_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd4ffd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the TF-IDF matrix later\n",
    "loaded_tfidf = joblib.load('tfidf_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f65dc516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_components = 50  # Adjust this value\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "X_svd = svd.fit_transform(loaded_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6070e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale the features to be in the range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d1cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, df['job_role'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Train classification models\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "\n",
    "# SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  \n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "# Print accuracy scores\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_logreg)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "print(\"Accuracy:\", accuracy_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64a34a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['job_role_encoded'] = le.fit_transform(df['job_role'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e34ae9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline on your training data\n",
    "X_train = X_scaled  # Assuming 'Skillset' is the column with skills\n",
    "y_train = df['job_role_encoded']   # Assuming 'job_role' is the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40177c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1000\n",
       "1    1000\n",
       "2    1000\n",
       "3    1000\n",
       "4    1000\n",
       "5     649\n",
       "Name: job_role_encoded, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['job_role_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "012a87a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5658, 5649]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# svm model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# SVM\u001b[39;00m\n\u001b[0;32m      3\u001b[0m svm \u001b[38;5;241m=\u001b[39m SVC()\n\u001b[1;32m----> 4\u001b[0m \u001b[43msvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:192\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    190\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    203\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m    204\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    205\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1124\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5658, 5649]"
     ]
    }
   ],
   "source": [
    "# svm model\n",
    "# SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe904e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and vectorizer as pickle files\n",
    "with open('svm_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(pipeline, model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1aa4ca",
   "metadata": {},
   "source": [
    "### LSTM & RNN & "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "af7796ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "142/142 [==============================] - 29s 126ms/step - loss: 1.8283 - accuracy: 0.1913 - val_loss: 1.7810 - val_accuracy: 0.2076\n",
      "Epoch 2/5\n",
      "142/142 [==============================] - 15s 108ms/step - loss: 1.7572 - accuracy: 0.2269 - val_loss: 1.7214 - val_accuracy: 0.2129\n",
      "Epoch 3/5\n",
      "142/142 [==============================] - 15s 103ms/step - loss: 1.6978 - accuracy: 0.2570 - val_loss: 1.6752 - val_accuracy: 0.2500\n",
      "Epoch 4/5\n",
      "142/142 [==============================] - 15s 104ms/step - loss: 1.7180 - accuracy: 0.2680 - val_loss: 1.8138 - val_accuracy: 0.2014\n",
      "Epoch 5/5\n",
      "142/142 [==============================] - 16s 116ms/step - loss: 1.7599 - accuracy: 0.2589 - val_loss: 1.7318 - val_accuracy: 0.2712\n",
      "36/36 [==============================] - 5s 51ms/step\n",
      "Accuracy: 0.2712014134275618\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Skillset'], df['job_role'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 1000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_length = 100  # You can adjust this based on your data\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=32, input_length=max_sequence_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train_encoded, epochs=5, validation_data=(X_test_padded, y_test_encoded))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "y_pred_probs = model.predict(X_test_padded)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "accuracy_lstm = accuracy_score(y_test_encoded, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "766d9570",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m max_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(num_words\u001b[38;5;241m=\u001b[39mmax_words)\n\u001b[1;32m---> 15\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(\u001b[43mtexts\u001b[49m)\n\u001b[0;32m     16\u001b[0m sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(texts)\n\u001b[0;32m     17\u001b[0m X \u001b[38;5;241m=\u001b[39m pad_sequences(sequences)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_words = 1000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "embedding_dim = 50\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=X.shape[1]))\n",
    "model.add(SimpleRNN(units=50))  # You can adjust the number of units based on your dataset\n",
    "model.add(Dense(units=len(np.unique(y)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict_classes(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6bf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import issparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74afd5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator TruncatedSVD from version 1.4.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.4.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator SVC from version 1.4.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator Pipeline from version 1.4.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.4.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained SVM model and TF-IDF vectorizer\n",
    "with open('svm_model.pkl', 'rb') as model_file:\n",
    "    svm_model = pickle.load(model_file)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as vectorizer_file:\n",
    "    tfidf_vectorizer = pickle.load(vectorizer_file)\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Load the fitted LabelEncoder from training\n",
    "with open('label_encoder.pkl', 'rb') as label_encoder_file:\n",
    "    le.classes_ = pickle.load(label_encoder_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "694720aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'LabelEncoder' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m prediction \u001b[38;5;241m=\u001b[39m svm_model\u001b[38;5;241m.\u001b[39mpredict(user_input)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Map the prediction to a job category or skillset (customize as needed)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#job_category = map_prediction_to_category(prediction)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:160\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 160\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(y, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(diff):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(diff))\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'LabelEncoder' has no len()"
     ]
    }
   ],
   "source": [
    "user_input = [\"python\"]\n",
    "\n",
    "\n",
    "# Make a prediction using the SVM model\n",
    "prediction = svm_model.predict(user_input)\n",
    "\n",
    "# Map the prediction to a job category or skillset (customize as needed)\n",
    "#job_category = map_prediction_to_category(prediction)\n",
    "predicted_label = le.inverse_transform(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "301a82d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75b7910e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'LabelEncoder' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:160\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 160\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(y, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(diff):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(diff))\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'LabelEncoder' has no len()"
     ]
    }
   ],
   "source": [
    "le.inverse_transform([4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d45ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
