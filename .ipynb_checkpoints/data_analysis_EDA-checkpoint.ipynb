{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aad809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5807dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/concatenated_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44b6b853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>country_code</th>\n",
       "      <th>current_company:company_id</th>\n",
       "      <th>current_company:name</th>\n",
       "      <th>position</th>\n",
       "      <th>followers</th>\n",
       "      <th>about</th>\n",
       "      <th>...</th>\n",
       "      <th>education</th>\n",
       "      <th>avatar</th>\n",
       "      <th>languages</th>\n",
       "      <th>certifications</th>\n",
       "      <th>recommendations</th>\n",
       "      <th>recommendations_count</th>\n",
       "      <th>volunteer_experience</th>\n",
       "      <th>courses</th>\n",
       "      <th>connections</th>\n",
       "      <th>job_role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>sheharyar-chaudhary-325b3465</td>\n",
       "      <td>Sheharyar Chaudhary</td>\n",
       "      <td>Dallas, Texas, United States</td>\n",
       "      <td>US</td>\n",
       "      <td>avantlink</td>\n",
       "      <td>AvantLink</td>\n",
       "      <td>Data Engineer || Google Certified Data Analyst...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>With over 7 years of extensive hands-on experi...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"degree\":\"Bachelor of Science (BS)\",\"end_yea...</td>\n",
       "      <td>https://static.licdn.com/aero-v1/sc/h/1c5u578i...</td>\n",
       "      <td>[{\"subtitle\":\"Full professional proficiency\",\"...</td>\n",
       "      <td>[{\"meta\":\"Issued Jun 2023 See credential\",\"sub...</td>\n",
       "      <td>Umer Siddiqui “I had the privilege of working ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>byby-louise-a91456243</td>\n",
       "      <td>Byby Louise</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "      <td>US</td>\n",
       "      <td>fiscalnote</td>\n",
       "      <td>FiscalNote</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"degree\":\"Master's degree\",\"end_year\":\"2014\"...</td>\n",
       "      <td>https://media.licdn.com/dms/image/C4E03AQEYsL5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>larry-richardson-jr</td>\n",
       "      <td>Larry Richardson Jr.</td>\n",
       "      <td>Frederick, Maryland, United States</td>\n",
       "      <td>US</td>\n",
       "      <td>ncma</td>\n",
       "      <td>National Contract Management Association (NCMA)</td>\n",
       "      <td>Infrastructure as Code | Data Analysis | Data ...</td>\n",
       "      <td>243.0</td>\n",
       "      <td>I caught fire with cloud computing. About eigh...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"degree\":\"Data Analytics/Engineering\",\"meta\"...</td>\n",
       "      <td>https://static.licdn.com/aero-v1/sc/h/1c5u578i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"meta\":\"Issued Sep 2023\",\"subtitle\":\"Associa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-13</td>\n",
       "      <td>thomas-roussos</td>\n",
       "      <td>Thomas Roussos</td>\n",
       "      <td>Massapequa</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Student at State University of New York Colleg...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Hello! My name is Thomas Roussos and I am a st...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"end_year\":\"2018\",\"start_year\":\"2014\",\"title...</td>\n",
       "      <td>https://media.licdn.com/dms/image/C5603AQGviES...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-12-13</td>\n",
       "      <td>oceanox</td>\n",
       "      <td>XIN OU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>transsnetfinancial</td>\n",
       "      <td>Transsnet Financial</td>\n",
       "      <td>Risk Control Data Analyst at Transsnet Financi...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>I am an MBA in Business Analytics student who ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"degree\":\"Master of Business Administration ...</td>\n",
       "      <td>https://media.licdn.com/dms/image/C4D03AQEvZ-L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp                            id                  name  \\\n",
       "0  2023-11-06  sheharyar-chaudhary-325b3465   Sheharyar Chaudhary   \n",
       "1  2023-11-06         byby-louise-a91456243           Byby Louise   \n",
       "2  2023-11-06           larry-richardson-jr  Larry Richardson Jr.   \n",
       "3  2023-12-13                thomas-roussos        Thomas Roussos   \n",
       "4  2023-12-13                       oceanox                XIN OU   \n",
       "\n",
       "                                 city country_code current_company:company_id  \\\n",
       "0        Dallas, Texas, United States           US                  avantlink   \n",
       "1   New York, New York, United States           US                 fiscalnote   \n",
       "2  Frederick, Maryland, United States           US                       ncma   \n",
       "3                          Massapequa           US                        NaN   \n",
       "4                                 NaN           US         transsnetfinancial   \n",
       "\n",
       "                              current_company:name  \\\n",
       "0                                        AvantLink   \n",
       "1                                       FiscalNote   \n",
       "2  National Contract Management Association (NCMA)   \n",
       "3                                              NaN   \n",
       "4                              Transsnet Financial   \n",
       "\n",
       "                                            position  followers  \\\n",
       "0  Data Engineer || Google Certified Data Analyst...     2000.0   \n",
       "1                                       Data Analyst      112.0   \n",
       "2  Infrastructure as Code | Data Analysis | Data ...      243.0   \n",
       "3  Student at State University of New York Colleg...       15.0   \n",
       "4  Risk Control Data Analyst at Transsnet Financi...       31.0   \n",
       "\n",
       "                                               about  ...  \\\n",
       "0  With over 7 years of extensive hands-on experi...  ...   \n",
       "1                                                NaN  ...   \n",
       "2  I caught fire with cloud computing. About eigh...  ...   \n",
       "3  Hello! My name is Thomas Roussos and I am a st...  ...   \n",
       "4  I am an MBA in Business Analytics student who ...  ...   \n",
       "\n",
       "                                           education  \\\n",
       "0  [{\"degree\":\"Bachelor of Science (BS)\",\"end_yea...   \n",
       "1  [{\"degree\":\"Master's degree\",\"end_year\":\"2014\"...   \n",
       "2  [{\"degree\":\"Data Analytics/Engineering\",\"meta\"...   \n",
       "3  [{\"end_year\":\"2018\",\"start_year\":\"2014\",\"title...   \n",
       "4  [{\"degree\":\"Master of Business Administration ...   \n",
       "\n",
       "                                              avatar  \\\n",
       "0  https://static.licdn.com/aero-v1/sc/h/1c5u578i...   \n",
       "1  https://media.licdn.com/dms/image/C4E03AQEYsL5...   \n",
       "2  https://static.licdn.com/aero-v1/sc/h/1c5u578i...   \n",
       "3  https://media.licdn.com/dms/image/C5603AQGviES...   \n",
       "4  https://media.licdn.com/dms/image/C4D03AQEvZ-L...   \n",
       "\n",
       "                                           languages  \\\n",
       "0  [{\"subtitle\":\"Full professional proficiency\",\"...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                      certifications  \\\n",
       "0  [{\"meta\":\"Issued Jun 2023 See credential\",\"sub...   \n",
       "1                                                NaN   \n",
       "2  [{\"meta\":\"Issued Sep 2023\",\"subtitle\":\"Associa...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                     recommendations recommendations_count  \\\n",
       "0  Umer Siddiqui “I had the privilege of working ...                   9.0   \n",
       "1                                                NaN                   NaN   \n",
       "2                                                NaN                   NaN   \n",
       "3                                                NaN                   NaN   \n",
       "4                                                NaN                   NaN   \n",
       "\n",
       "  volunteer_experience courses connections       job_role  \n",
       "0                  NaN     NaN         NaN  Data Analysis  \n",
       "1                  NaN     NaN         NaN  Data Analysis  \n",
       "2                  NaN     NaN         NaN  Data Analysis  \n",
       "3                  NaN     NaN        15.0  Data Analysis  \n",
       "4                  NaN     NaN        31.0  Data Analysis  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09611ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data Analysis                  1000\n",
       "Manager                        1000\n",
       "Project Manager                1000\n",
       "Research & Development         1000\n",
       "Software Engineer              1000\n",
       "UI/UX designer                  649\n",
       "Quatity Assurance & Testing       8\n",
       "System & Network Engineer         1\n",
       "Name: job_role, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['job_role'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "130159fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['about', 'experience','certifications','job_role']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93db6ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>experience</th>\n",
       "      <th>certifications</th>\n",
       "      <th>job_role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With over 7 years of extensive hands-on experi...</td>\n",
       "      <td>[{\"description\":\"As a Data Engineer at AvantLi...</td>\n",
       "      <td>[{\"meta\":\"Issued Jun 2023 See credential\",\"sub...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"description\":\"Use data mining software and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I caught fire with cloud computing. About eigh...</td>\n",
       "      <td>[{\"duration\":\"Apr 2023 - Present 8 months\",\"du...</td>\n",
       "      <td>[{\"meta\":\"Issued Sep 2023\",\"subtitle\":\"Associa...</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello! My name is Thomas Roussos and I am a st...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am an MBA in Business Analytics student who ...</td>\n",
       "      <td>[{\"description\":\"Shenzhen, Guangdong, China - ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Analysis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               about  \\\n",
       "0  With over 7 years of extensive hands-on experi...   \n",
       "1                                                NaN   \n",
       "2  I caught fire with cloud computing. About eigh...   \n",
       "3  Hello! My name is Thomas Roussos and I am a st...   \n",
       "4  I am an MBA in Business Analytics student who ...   \n",
       "\n",
       "                                          experience  \\\n",
       "0  [{\"description\":\"As a Data Engineer at AvantLi...   \n",
       "1  [{\"description\":\"Use data mining software and ...   \n",
       "2  [{\"duration\":\"Apr 2023 - Present 8 months\",\"du...   \n",
       "3                                                NaN   \n",
       "4  [{\"description\":\"Shenzhen, Guangdong, China - ...   \n",
       "\n",
       "                                      certifications       job_role  \n",
       "0  [{\"meta\":\"Issued Jun 2023 See credential\",\"sub...  Data Analysis  \n",
       "1                                                NaN  Data Analysis  \n",
       "2  [{\"meta\":\"Issued Sep 2023\",\"subtitle\":\"Associa...  Data Analysis  \n",
       "3                                                NaN  Data Analysis  \n",
       "4                                                NaN  Data Analysis  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5afa6863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "about             2139\n",
       "experience         221\n",
       "certifications    4354\n",
       "job_role             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66d5d647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5658 entries, 0 to 5657\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   about           3519 non-null   object\n",
      " 1   experience      5437 non-null   object\n",
      " 2   certifications  1304 non-null   object\n",
      " 3   job_role        5658 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 176.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd84d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have 5650 data records\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8256ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new 'Skillset' column\n",
    "df1['skillset'] = df1['about'].fillna('') + ' ' + df1['experience'].fillna('') + ' ' + df1['certifications'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21612afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0aaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0240bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_skills(text):\n",
    "    doc = nlp(text)\n",
    "    # Extract unique lemmatized tokens as skills\n",
    "    #skills = list(set(token.lemma_ for token in doc if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\"))\n",
    "    ner_skills = [ent.text for ent in doc.ents if ent.label_ == \"SKILL\"]\n",
    "    print(ner_skills)\n",
    "    return ner_skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "235ee644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5658, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop all the records with skillset as null\n",
    "df1.dropna(subset = [\"skillset\"])\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0afebf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIMAN\\AppData\\Local\\Temp\\ipykernel_21876\\2945025637.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df1[\"skillset\"] = df1[\"skillset\"].str.replace(r'[^a-zA-Z0-9\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df1[\"skillset\"] = df1[\"skillset\"].str.replace(r'[^a-zA-Z0-9\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "254c7eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    []\n",
       "1    []\n",
       "2    []\n",
       "3    []\n",
       "4    []\n",
       "5    []\n",
       "Name: skillset, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.loc[0:5,\"skillset\"].apply(extract_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "053ba980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    With over 7 years of extensive hands-on experi...\n",
       "1     [{\"description\":\"Use data mining software and...\n",
       "2    I caught fire with cloud computing. About eigh...\n",
       "3    Hello! My name is Thomas Roussos and I am a st...\n",
       "4    I am an MBA in Business Analytics student who ...\n",
       "5     [{\"company\":\"House of Providence\",\"company_id...\n",
       "Name: skillset, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.loc[0:5,\"skillset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14cd768d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"I have experience in Python, Java, and machine learning.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract skills using NER\n",
    "skills = [ent.text for ent in doc.ents if ent.label_ == \"SKILL\"]\n",
    "\n",
    "# Print the extracted skills\n",
    "print(\"Extracted Skills:\", skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99290f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the extract_skills function to each row in the 'Skillset' column\n",
    "df['skillset'] = df['skillset'].apply(extract_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea3c428d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['running', 'runs', 'ran', 'coding', ',', 'code', ',', 'i']\n",
      "Lemmatized: ['running', 'run', 'ran', 'coding', ',', 'code', ',', 'i']\n",
      "Stemmed: ['run', 'run', 'ran', 'code', ',', 'code', ',', 'i']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Lemmatization aims to reduce a word to its base or root form, known as the lemma. The lemma represents the canonical, dictionary form of a word.\n",
    "It involves considering the context of the word and converting it to its base form, which often corresponds to a valid word.\"\"\"\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"running runs ran coding, code, i \"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Stemming-- performes better\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Lemmatized:\", lemmatized_words)\n",
    "print(\"Stemmed:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28ea9f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens (without stop words): ['experience', 'Python', ',', 'Java', ',', 'machine', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "# don't want to seperate 1 skillds like machine learning into 2\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"I have experience in Python, Java, and machine learning.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Print the filtered tokens\n",
    "print(\"Filtered Tokens (without stop words):\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "679d91cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy\n",
    "from spacy.pipeline.textcat import Config, single_label_cnn_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "10269a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIMAN\\AppData\\Local\\Temp\\ipykernel_21876\\3929097957.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns= {'skillset': 'Skillset'},inplace = True)\n"
     ]
    }
   ],
   "source": [
    "df = df1[['job_role','skillset']]\n",
    "df.rename(columns= {'skillset': 'Skillset'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "21c78e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIMAN\\AppData\\Local\\Temp\\ipykernel_21876\\1614861931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Skillset'] = df['Skillset'].astype(str)\n",
      "C:\\Users\\AIMAN\\AppData\\Local\\Temp\\ipykernel_21876\\1614861931.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Skillset'] = df['Skillset'].str.replace('[^a-zA-Z\\s]', '').str.lower()\n",
      "C:\\Users\\AIMAN\\AppData\\Local\\Temp\\ipykernel_21876\\1614861931.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Skillset'] = df['Skillset'].str.replace('[^a-zA-Z\\s]', '').str.lower()\n",
      "C:\\Users\\AIMAN\\AppData\\Local\\Temp\\ipykernel_21876\\1614861931.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Skillset'] = df['Skillset'].apply(remove_stopwords)\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'Skillset' column to string\n",
    "df['Skillset'] = df['Skillset'].astype(str)\n",
    "\n",
    "# Text preprocessing (remove special characters, lowercase, etc.)\n",
    "df['Skillset'] = df['Skillset'].str.replace('[^a-zA-Z\\s]', '').str.lower()\n",
    "\n",
    "# Remove stop words\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if token.text not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['Skillset'] = df['Skillset'].apply(remove_stopwords)\n",
    "\n",
    "# Vectorize the 'Skillset' column\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['Skillset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "06a668fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_components = 50  # Adjust this value\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "X_svd = svd.fit_transform(loaded_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e179ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale the features to be in the range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d3a167c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIMAN\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Naive Bayes\u001b[39;00m\n\u001b[0;32m     17\u001b[0m nb \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[1;32m---> 18\u001b[0m \u001b[43mnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m y_pred_nb \u001b[38;5;241m=\u001b[39m nb\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     20\u001b[0m accuracy_nb \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred_nb)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:776\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    774\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[1;32m--> 776\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:898\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 898\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultinomialNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1418\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1415\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, df['job_role'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Train classification models\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "\n",
    "# SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# Print accuracy scores\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_logreg)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6f552dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_matrix.pkl']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save TF-IDF matrix to a file\n",
    "joblib.dump(X, 'tfidf_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1cd62f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\aiman\\anaconda3\\lib\\site-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09c384c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the TF-IDF matrix later\n",
    "loaded_tfidf = joblib.load('tfidf_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550aab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
